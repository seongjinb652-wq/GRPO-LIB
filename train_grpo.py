# ============================================
# ğŸ“Œ train_grpo.py
# ëª©ì : GRPOTrainerë¥¼ ì´ìš©í•´ ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
# - í•™ìŠµ ì‹œì‘/ì™„ë£Œ ë¡œê·¸ ì¶œë ¥
# - í•™ìŠµ ê²°ê³¼ (ìŠ¤í…, Loss) í™•ì¸ ê°€ëŠ¥
# - ì˜ˆì™¸ ì²˜ë¦¬: ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì • ì•ˆë‚´
# ============================================

# GRPOTrainer ìƒì„± ë° í•™ìŠµ
print("\nğŸš€ GRPO í•™ìŠµ ì‹œì‘")
print("=" * 50)

# Trainer ìƒì„±
trainer = GRPOTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    processing_class=tokenizer,
    reward_funcs=math_reward_function,
)

# í•™ìŠµ ì‹¤í–‰
print("í•™ìŠµ ì§„í–‰ ì¤‘... (ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”)")
print("-" * 50)

try:
    train_result = trainer.train()

    print("\n" + "=" * 50)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"   ì´ ìŠ¤í…: {train_result.global_step}")
    print(f"   ìµœì¢… Loss: {train_result.training_loss:.4f}")

except Exception as e:
    print(f"\nâš ï¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("ë©”ëª¨ë¦¬ ë¶€ì¡±ì¼ ê²½ìš° ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.")
