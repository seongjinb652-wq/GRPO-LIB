# ============================================
# ğŸ“Œ load_model.py
# ëª©ì : í•™ìŠµ/ì¶”ë¡ ì— ì‚¬ìš©í•  ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
# - CONFIG["model_name"] ê¸°ë°˜ìœ¼ë¡œ AutoTokenizer, AutoModelForCausalLM ë¶ˆëŸ¬ì˜¤ê¸°
# - ì •ë°€ë„(fp16, bf16, fp32) ì„¤ì •
# - ì„ íƒì ìœ¼ë¡œ LoRA ì ìš© (CONFIG["use_lora"])
# ============================================

# ëª¨ë¸ ë¡œë“œ
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
import torch

print("ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
print("=" * 50)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(CONFIG["model_name"])
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"  # ìƒì„± ëª¨ë¸ì— ê¶Œì¥

print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {CONFIG['model_name']}")

# ëª¨ë¸ ë¡œë“œ (ì •ë°€ë„ ì„¤ì •)
if CONFIG["precision"] == "bf16":
    dtype = torch.bfloat16
elif CONFIG["precision"] == "fp16":
    dtype = torch.float16
else:
    dtype = torch.float32

model = AutoModelForCausalLM.from_pretrained(
    CONFIG["model_name"],
    torch_dtype=dtype,
    device_map="auto",
    trust_remote_code=True,
)

print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
print(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters() / 1e6:.1f}M")

# LoRA ì ìš©
if CONFIG["use_lora"]:
    lora_config = LoraConfig(
        r=16,                          # LoRA rank
        lora_alpha=32,                 # ìŠ¤ì¼€ì¼ë§ íŒ©í„°
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora_config)

    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())

    print(f"\nâœ… LoRA ì ìš© ì™„ë£Œ")
    print(f"   í•™ìŠµ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
