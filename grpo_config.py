# GRPO ν•™μµ μ„¤μ •
from trl import GRPOConfig, GRPOTrainer

print("π”§ GRPO ν•™μµ μ„¤μ • μ¤‘...")
print("=" * 50)

# GRPOConfig μ„¤μ •
training_args = GRPOConfig(
    output_dir="./grpo_math_model",

    # ν•™μµ νλΌλ―Έν„°
    max_steps=CONFIG["max_steps"],
    per_device_train_batch_size=CONFIG["batch_size"],
    gradient_accumulation_steps=4,
    learning_rate=CONFIG["learning_rate"],
    warmup_ratio=0.1,

    # GRPO νΉν™” νλΌλ―Έν„°
    num_generations=CONFIG["num_generations"],
    max_completion_length=256,
    max_prompt_length=256,
    temperature=0.9,
    beta=0.04,

    # λ΅κΉ… λ° μ €μ¥
    logging_steps=5,
    save_steps=25,
    save_total_limit=2,

    # λ©”λ¨λ¦¬ μµμ ν™”
    bf16=(CONFIG["precision"] == "bf16"),
    fp16=(CONFIG["precision"] == "fp16"),
    gradient_checkpointing=True,
    optim="adamw_torch_fused" if torch.cuda.is_available() else "adamw_torch",

    # κΈ°νƒ€
    remove_unused_columns=False,
    seed=42,
)

print("β… GRPOConfig μ„¤μ • μ™„λ£")
print(f"   max_steps: {training_args.max_steps}")
print(f"   batch_size: {training_args.per_device_train_batch_size}")
print(f"   num_generations: {training_args.num_generations}")
