# ============================================
# ğŸ“Œ grpo_pipeline_demo.py
# ëª©ì : GRPO í•™ìŠµ íŒŒì´í”„ë¼ì¸ ë°ëª¨ ì½”ë“œ
# - ëª¨ë¸ ë¡œë“œ, LoRA ì ìš©, ë³´ìƒ í•¨ìˆ˜ ì •ì˜, Config ì„¤ì •, Trainer ìƒì„±, í•™ìŠµ ì‹¤í–‰, ì €ì¥ê¹Œì§€ í¬í•¨
# - ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ëª¨ë“ˆí™”ëœ êµ¬ì¡°(run_grpo_pipeline.py) ì‚¬ìš© ê¶Œì¥
# ============================================
# ğŸ“ ì™„ì „í•œ GRPO í•™ìŠµ íŒŒì´í”„ë¼ì¸ (ë°ëª¨ìš© - )

def create_grpo_pipeline_demo():
    """
    GRPO í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë°ëª¨

    ì´ í•¨ìˆ˜ëŠ” íŒŒì´í”„ë¼ì¸ì˜ êµ¬ì¡°ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ê²ƒìœ¼ë¡œ,
    ì‹¤ì œ í•™ìŠµì€ ë‹¤ìŒ êµì‹œì—ì„œ ì§„í–‰í•©ë‹ˆë‹¤.
    """

    pipeline_code = '''
# === 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ===
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model

# ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (ì‘ì€ ëª¨ë¸ë¡œ ì‹œì‘)
model_name = "Qwen/Qwen2.5-0.5B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # ë©”ëª¨ë¦¬ íš¨ìœ¨
    device_map="auto",
)

# LoRA ì ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  íŒŒì¸íŠœë‹)
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, lora_config)

# === 2. ë³´ìƒ í•¨ìˆ˜ ì •ì˜ ===
def reward_function(completions, prompts, answer, **kwargs):
    """ìˆ˜í•™ ë¬¸ì œ ì •ë‹µ + í˜•ì‹ ë³´ìƒ"""
    rewards = []
    for completion, ans in zip(completions, answer):
        # ì •ë‹µ í™•ì¸
        numbers = re.findall(r"-?\d+", completion)
        correct = 1.0 if numbers and numbers[-1] == ans else 0.0

        # í˜•ì‹ í™•ì¸
        has_think = 0.2 if "<think>" in completion else 0.0

        rewards.append(correct + has_think)
    return rewards

# === 3. GRPOConfig ì„¤ì • ===
from trl import GRPOConfig

training_args = GRPOConfig(
    output_dir="./grpo_math_model",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=5e-6,
    num_generations=4,
    max_completion_length=256,
    beta=0.04,
    logging_steps=10,
    save_steps=100,
    bf16=True,
    gradient_checkpointing=True,
)

# === 4. GRPOTrainer ìƒì„± ===
from trl import GRPOTrainer

trainer = GRPOTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    processing_class=tokenizer,
    reward_funcs=reward_function,
)

# === 5. í•™ìŠµ ì‹¤í–‰ ===
trainer.train()

# === 6. ëª¨ë¸ ì €ì¥ ===
trainer.save_model("./grpo_math_model_final")
tokenizer.save_pretrained("./grpo_math_model_final")
'''

    return pipeline_code

# íŒŒì´í”„ë¼ì¸ ì½”ë“œ ì¶œë ¥
print("ğŸ”„ GRPO í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì„±")
print("=" * 60)
print(create_grpo_pipeline_demo())
print("=" * 60)

print("\n" + "=" * 60)
