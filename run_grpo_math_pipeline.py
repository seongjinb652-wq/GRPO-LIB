# ============================================
# ğŸ“Œ run_grpo_pipeline.py
# ëª©ì : GRPO í•™ìŠµ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# - ë°ì´í„°ì…‹ ìƒì„±
# - ë³´ìƒ í•¨ìˆ˜ ì •ì˜
# - í•™ìŠµ ì„¤ì • ë¡œë“œ
# - ëª¨ë¸ ë¡œë“œ ë° LoRA ì ìš©
# - GRPOTrainer í•™ìŠµ ì‹¤í–‰
# - ëª¨ë¸ ì €ì¥ ë° Hugging Face ì—…ë¡œë“œ
# - ì¶”ë¡  ì‘ë‹µ í…ŒìŠ¤íŠ¸
# ============================================

from generate_math_dataset import generate_math_problems
from math_reward import math_reward_function
from grpo_config import training_args
from load_model_with_lora import model, tokenizer
from train_grpo import GRPOTrainer
from model_saver import save_model_and_tokenizer
from huggingface_upload import upload_to_hub
from inference_response import generate_response

print("\nğŸš€ GRPO íŒŒì´í”„ë¼ì¸ ì‹œì‘")
print("=" * 50)

# 0. í°íŠ¸ ì„¤ì •. 
# 
# 1. ë°ì´í„°ì…‹ ì¤€ë¹„
train_dataset = generate_math_problems(200)

# 2. Trainer ìƒì„±
trainer = GRPOTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    processing_class=tokenizer,
    reward_funcs=math_reward_function,
)

# 3. í•™ìŠµ ì‹¤í–‰
train_result = trainer.train()
print(f"âœ… í•™ìŠµ ì™„ë£Œ: step={train_result.global_step}, loss={train_result.training_loss:.4f}")

# 4. ëª¨ë¸ ì €ì¥
save_model_and_tokenizer(trainer, tokenizer, "./grpo_math_model")

# 5. Hugging Face Hub ì—…ë¡œë“œ (ì„ íƒ)
# upload_to_hub("./grpo_math_model")

# 6. ì¶”ë¡  ì‘ë‹µ í…ŒìŠ¤íŠ¸
sample_prompt = "ë‹¤ìŒì„ ê³„ì‚°í•˜ì„¸ìš”: 12 + 7 = ?"
response = generate_response(model, tokenizer, sample_prompt)
print("\nğŸ“ ì¶”ë¡  ì‘ë‹µ ì˜ˆì‹œ:")
print(response)
