# ============================================
# ğŸ“Œ memory_optimized_config.py
# ëª©ì : GPU ë©”ëª¨ë¦¬ í¬ê¸°ì— ë”°ë¼ í•™ìŠµ ì„¤ì • ìë™ ìµœì í™”
# - A100, L4, T4, RTX ë“± í™˜ê²½ë³„ ê¶Œì¥ íŒŒë¼ë¯¸í„° ì œê³µ
# - ë°°ì¹˜ í¬ê¸°, ê·¸ë£¹ í¬ê¸°, ì •ë°€ë„(fp16/bf16), LoRA ì‚¬ìš© ì—¬ë¶€, ëª¨ë¸ í¬ê¸° ì¡°ì •
# - í˜„ì¬ í™˜ê²½ì„ ê°ì§€í•˜ì—¬ ê¶Œì¥ ì„¤ì • ì¶œë ¥
# ============================================
# ğŸ“ ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì˜ˆì‹œ
def get_optimized_config(gpu_memory_gb: float) -> dict:
    """
    GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì í™” ì„¤ì • ë°˜í™˜

    Args:
        gpu_memory_gb: GPU ë©”ëª¨ë¦¬ (GB)

    Returns:
        ìµœì í™”ëœ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    if gpu_memory_gb >= 40:  # A100 40GB+
        config = {
            "batch_size": 8,
            "num_generations": 16,
            "precision": "bf16",
            "lora": False,
            "gradient_checkpointing": False,
            "model_size": "7B",
        }
    elif gpu_memory_gb >= 20:  # L4 24GB
        config = {
            "batch_size": 4,
            "num_generations": 8,
            "precision": "bf16",
            "lora": True,
            "gradient_checkpointing": True,
            "model_size": "1.5B~3B",
        }
    elif gpu_memory_gb >= 14:  # T4 16GB
        config = {
            "batch_size": 2,
            "num_generations": 4,
            "precision": "fp16",
            "lora": True,
            "gradient_checkpointing": True,
            "model_size": "0.5B~1.5B",
        }
    else:  # ì‘ì€ GPU
        config = {
            "batch_size": 1,
            "num_generations": 2,
            "precision": "fp16",
            "lora": True,
            "gradient_checkpointing": True,
            "model_size": "0.5B",
        }

    return config

# í˜„ì¬ í™˜ê²½ì— ë§ëŠ” ì„¤ì • ì¶œë ¥
print("ğŸ§  GPU ë©”ëª¨ë¦¬ë³„ ìµœì í™” ì„¤ì •")
print("=" * 60)

gpu_options = [
    (40, "A100 40GB"),
    (24, "L4 24GB"),
    (16, "T4 16GB"),
    (8, "RTX 3070 8GB"),
]

for mem, name in gpu_options:
    config = get_optimized_config(mem)
    print(f"\nğŸ“Œ {name}:")
    print(f"   ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
    print(f"   ê·¸ë£¹ í¬ê¸°: {config['num_generations']}")
    print(f"   ì •ë°€ë„: {config['precision']}")
    print(f"   LoRA ì‚¬ìš©: {'âœ…' if config['lora'] else 'âŒ'}")
    print(f"   ê¶Œì¥ ëª¨ë¸: {config['model_size']}")

# í˜„ì¬ í™˜ê²½
if torch.cuda.is_available():
    current_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"\n{'='*60}")
    print(f"ğŸ–¥ï¸ í˜„ì¬ í™˜ê²½: {torch.cuda.get_device_name(0)} ({current_mem:.0f}GB)")
    current_config = get_optimized_config(current_mem)
    print(f"   â†’ ê¶Œì¥ ì„¤ì •: ë°°ì¹˜={current_config['batch_size']}, "
          f"ê·¸ë£¹={current_config['num_generations']}, "
          f"ëª¨ë¸={current_config['model_size']}")
